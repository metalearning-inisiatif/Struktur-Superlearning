{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Super Learning Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMX0AN3w5Q6lVwmmqQm7Qu0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metalearning-inisiatif/Struktur-Superlearning/blob/main/STRUKTUR%20META%20LEARNING/Super_Learning_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBBoZjZH043h"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Super Learner for Classification**\n",
        "\n",
        "The super learner algorithm for classification is much the same.\n",
        "\n",
        "The inputs to the meta learner can be class labels or class probabilities, with the latter more likely to be useful given the increased granularity or uncertainty captured in the predictions.\n",
        "\n",
        "In this problem, we will use the make_blobs() test classification problem and use 1,000 examples with 100 input variables and two class labels."
      ],
      "metadata": {
        "id": "IeN3VgCv1k7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a super learner model for binary classification\n",
        "from numpy import hstack\n",
        "from numpy import vstack\n",
        "from numpy import asarray\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier"
      ],
      "metadata": {
        "id": "6CPO9CuP1-_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "# create the inputs and outputs\n",
        "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
        "# split\n",
        "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
        "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiQnWg8i2H0p",
        "outputId": "15b87048-006a-49d6-8ec5-a868100c7384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (500, 100) (500,) Test (500, 100) (500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can change the get_models() function to define a suite of linear and nonlinear classification algorithms."
      ],
      "metadata": {
        "id": "KDGxb6a02YOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of base-models\n",
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
        "\tmodels.append(DecisionTreeClassifier())\n",
        "\tmodels.append(SVC(gamma='scale', probability=True))\n",
        "\tmodels.append(GaussianNB())\n",
        "\tmodels.append(KNeighborsClassifier())\n",
        "\tmodels.append(AdaBoostClassifier())\n",
        "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
        "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
        "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
        "\treturn models"
      ],
      "metadata": {
        "id": "7tyZ2gRN2csx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can change the get_out_of_fold_predictions() function to predict probabilities by a call to the predict_proba() function."
      ],
      "metadata": {
        "id": "svFZOgdN2l6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect out of fold predictions form k-fold cross validation\n",
        "def get_out_of_fold_predictions(X, y, models):\n",
        "\tmeta_X, meta_y = list(), list()\n",
        "\t# define split of data\n",
        "\tkfold = KFold(n_splits=10, shuffle=True)\n",
        "\t# enumerate splits\n",
        "\tfor train_ix, test_ix in kfold.split(X):\n",
        "\t\tfold_yhats = list()\n",
        "\t\t# get data\n",
        "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
        "\t\tmeta_y.extend(test_y)\n",
        "\t\t# fit and make predictions with each sub-model\n",
        "\t\tfor model in models:\n",
        "\t\t\tmodel.fit(train_X, train_y)\n",
        "\t\t\tyhat = model.predict_proba(test_X)\n",
        "\t\t\t# store columns\n",
        "\t\t\tfold_yhats.append(yhat)\n",
        "\t\t# store fold yhats as columns\n",
        "\t\tmeta_X.append(hstack(fold_yhats))\n",
        "\treturn vstack(meta_X), asarray(meta_y)"
      ],
      "metadata": {
        "id": "aIK3OGbK2n9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Logistic Regression algorithm instead of a Linear Regression algorithm will be used as the meta-algorithm in the fit_meta_model() function."
      ],
      "metadata": {
        "id": "RKY10B-P21le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a meta model\n",
        "def fit_meta_model(X, y):\n",
        "\tmodel = LogisticRegression(solver='liblinear')\n",
        "\tmodel.fit(X, y)\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "McZ06BNE225B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And classification accuracy will be used to report model performance.\n",
        "\n",
        "The complete example of the super learner algorithm for classification using scikit-learn models is listed below."
      ],
      "metadata": {
        "id": "U6iI3ldW3EwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of base-models\n",
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
        "\tmodels.append(DecisionTreeClassifier())\n",
        "\tmodels.append(SVC(gamma='scale', probability=True))\n",
        "\tmodels.append(GaussianNB())\n",
        "\tmodels.append(KNeighborsClassifier())\n",
        "\tmodels.append(AdaBoostClassifier())\n",
        "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
        "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
        "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
        "\treturn models\n",
        "\n",
        "# collect out of fold predictions form k-fold cross validation\n",
        "def get_out_of_fold_predictions(X, y, models):\n",
        "\tmeta_X, meta_y = list(), list()\n",
        "\t# define split of data\n",
        "\tkfold = KFold(n_splits=10, shuffle=True)\n",
        "\t# enumerate splits\n",
        "\tfor train_ix, test_ix in kfold.split(X):\n",
        "\t\tfold_yhats = list()\n",
        "\t\t# get data\n",
        "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
        "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
        "\t\tmeta_y.extend(test_y)\n",
        "\t\t# fit and make predictions with each sub-model\n",
        "\t\tfor model in models:\n",
        "\t\t\tmodel.fit(train_X, train_y)\n",
        "\t\t\tyhat = model.predict_proba(test_X)\n",
        "\t\t\t# store columns\n",
        "\t\t\tfold_yhats.append(yhat)\n",
        "\t\t# store fold yhats as columns\n",
        "\t\tmeta_X.append(hstack(fold_yhats))\n",
        "\treturn vstack(meta_X), asarray(meta_y)\n",
        "\n",
        "# fit all base models on the training dataset\n",
        "def fit_base_models(X, y, models):\n",
        "\tfor model in models:\n",
        "\t\tmodel.fit(X, y)\n",
        "\n",
        "# fit a meta model\n",
        "def fit_meta_model(X, y):\n",
        "\tmodel = LogisticRegression(solver='liblinear')\n",
        "\tmodel.fit(X, y)\n",
        "\treturn model\n",
        "\n",
        "# evaluate a list of models on a dataset\n",
        "def evaluate_models(X, y, models):\n",
        "\tfor model in models:\n",
        "\t\tyhat = model.predict(X)\n",
        "\t\tacc = accuracy_score(y, yhat)\n",
        "\t\tprint('%s: %.3f' % (model.__class__.__name__, acc*100))\n",
        "\n",
        "# make predictions with stacked model\n",
        "def super_learner_predictions(X, models, meta_model):\n",
        "\tmeta_X = list()\n",
        "\tfor model in models:\n",
        "\t\tyhat = model.predict_proba(X)\n",
        "\t\tmeta_X.append(yhat)\n",
        "\tmeta_X = hstack(meta_X)\n",
        "\t# predict\n",
        "\treturn meta_model.predict(meta_X)\n",
        "\n",
        "# create the inputs and outputs\n",
        "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
        "# split\n",
        "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
        "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
        "# get models\n",
        "models = get_models()\n",
        "# get out of fold predictions\n",
        "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
        "print('Meta ', meta_X.shape, meta_y.shape)\n",
        "# fit base models\n",
        "fit_base_models(X, y, models)\n",
        "# fit the meta model\n",
        "meta_model = fit_meta_model(meta_X, meta_y)\n",
        "# evaluate base models\n",
        "evaluate_models(X_val, y_val, models)\n",
        "# evaluate meta model\n",
        "yhat = super_learner_predictions(X_val, models, meta_model)\n",
        "print('Super Learner: %.3f' % (accuracy_score(y_val, yhat) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPXCqk7H3H0n",
        "outputId": "3be19793-bb17-42c6-8c0e-5726fdada804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (500, 100) (500,) Test (500, 100) (500,)\n",
            "Meta  (500, 18) (500,)\n",
            "LogisticRegression: 97.200\n",
            "DecisionTreeClassifier: 67.600\n",
            "SVC: 98.200\n",
            "GaussianNB: 98.400\n",
            "KNeighborsClassifier: 94.000\n",
            "AdaBoostClassifier: 96.800\n",
            "BaggingClassifier: 85.600\n",
            "RandomForestClassifier: 87.600\n",
            "ExtraTreesClassifier: 84.000\n",
            "Super Learner: 98.400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, the shape of the dataset and the prepared meta dataset is reported, followed by the performance of the base-models on the holdout dataset and finally the super model itself on the holdout dataset.\n",
        "\n",
        "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
        "\n",
        "In this case, we can see that the super learner has slightly better performance than the base learner algorithms."
      ],
      "metadata": {
        "id": "KvCzmr3q34E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DLtqlyzf35iO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}